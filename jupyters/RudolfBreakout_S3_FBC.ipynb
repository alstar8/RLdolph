{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from rudalle import get_vae\n",
    "from rudalle.utils import seed_everything\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '../../deep_rl_zoo')\n",
    "sys.path.insert(0, '../src/rudolph')\n",
    "sys.path.insert(0, '../')\n",
    "\n",
    "import os\n",
    "from glob import glob\n",
    "from os.path import join\n",
    "from datetime import datetime\n",
    "import bitsandbytes as bnb\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "import transformers\n",
    "import more_itertools\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as T\n",
    "from tqdm.auto import tqdm\n",
    "from einops import rearrange\n",
    "import youtokentome as yttm\n",
    "import torchvision\n",
    "from rudalle.image_prompts import ImagePrompts\n",
    "\n",
    "from rudolph.model import get_rudolph_model, ruDolphModel, FP16Module\n",
    "from rudolph import utils\n",
    "from rudolph.model.utils import get_attention_mask\n",
    "from rudalle import get_tokenizer, get_vae\n",
    "from rudolph.api import ruDolphApi\n",
    "\n",
    "import datasets\n",
    "import random\n",
    "import h5py\n",
    "from PIL import Image\n",
    "import wandb\n",
    "\n",
    "\n",
    "from collections import deque\n",
    "import datetime\n",
    "import gym\n",
    "from gym.spaces import Box\n",
    "import cv2\n",
    "from deep_rl_zoo.networks.dqn import RainbowDqnMlpNet, RainbowDqnConvNet, R2d2DqnConvNet\n",
    "from deep_rl_zoo import main_loop\n",
    "from deep_rl_zoo.checkpoint import PyTorchCheckpoint\n",
    "from deep_rl_zoo import gym_env\n",
    "from deep_rl_zoo import greedy_actors\n",
    "import deep_rl_zoo.types as types_lib\n",
    "from moviepy.editor import ImageSequenceClip\n",
    "import pandas as pd\n",
    "import webdataset as wds\n",
    "import s3dataset\n",
    "import io\n",
    "from client_lib import save_aws_credentials,S3CopyJob\n",
    "import rudolph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze(\n",
    "    model,\n",
    "    freeze_emb=False,\n",
    "    freeze_ln=False,\n",
    "    freeze_attn=True,\n",
    "    freeze_ff=True,\n",
    "    freeze_other=False,\n",
    "):\n",
    "    for name, p in model.module.named_parameters():\n",
    "        name = name.lower()\n",
    "        if 'ln' in name or 'norm' in name:\n",
    "            p.requires_grad = not freeze_ln\n",
    "        elif 'embeddings' in name:\n",
    "            p.requires_grad = not freeze_emb\n",
    "        elif 'mlp' in name:\n",
    "            p.requires_grad = not freeze_ff\n",
    "        elif 'attn' in name:\n",
    "            p.requires_grad = not freeze_attn\n",
    "        else:\n",
    "            p.requires_grad = not freeze_other\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(0)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Russian Diffusion On Language Picture Hyper-modality (RuDOLPH ü¶åüéÑ‚òÉÔ∏è) 350M is a fast and light text-image-text transformer (350M GPT-3) designed for a quick and easy fine-tuning setup for the solution of various tasks: from generating images by text description and image classification to visual question answering and more. \n",
      "This model demonstrates the power of Hyper-modality Transformers.\n"
     ]
    }
   ],
   "source": [
    "model = get_rudolph_model('350M', pretrained=True, fp16=True, device=device) #2.7B #1.3B #350M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer --> ready\n",
      "Working with z of shape (1, 256, 32, 32) = 262144 dimensions.\n",
      "vae --> ready\n"
     ]
    }
   ],
   "source": [
    "tokenizer = get_tokenizer()\n",
    "vae = get_vae(dwt=False).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args():\n",
    "    def __init__(self, model, checkpoint_path):\n",
    "        self.device = model.get_param('device')\n",
    "        self.l_text_seq_length = model.get_param('l_text_seq_length')\n",
    "        self.r_text_seq_length = model.get_param('r_text_seq_length')\n",
    "        self.image_tokens_per_dim = model.get_param('image_tokens_per_dim')\n",
    "        self.image_seq_length = model.get_param('image_seq_length')\n",
    "        self.epochs = 1\n",
    "        self.save_path= checkpoint_path\n",
    "        self.model_name = 'rudolph_sberquad_'\n",
    "        self.save_every = 500\n",
    "        self.bs = 4\n",
    "        self.clip = 1.0\n",
    "        self.lr = 2e-5\n",
    "        self.wandb = False\n",
    "        self.lt_loss_weight = 0.0\n",
    "        self.img_loss_weight = 0.0\n",
    "        self.rt_loss_weight = 7\n",
    "        self.image_size = self.image_tokens_per_dim * 8\n",
    "        \n",
    "checkpoint_path = '../checkpoints/350_S3_FBC/'\n",
    "args = Args(model, checkpoint_path)\n",
    "args.bs = 48 #batch_size 4 default ##################################################################\n",
    "args.epochs = 10\n",
    "args.wandb = False\n",
    "if not os.path.exists(args.save_path):\n",
    "    os.makedirs(args.save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPC_TOKENS = {\n",
    "    '<LT_UNK>': 16384,\n",
    "    '<RT_UNK>': 16385,\n",
    "    '<LT_T2I>': 16386,\n",
    "    '<LT_I2T>': 16387,\n",
    "    '<LT_T2T>': 16388,\n",
    "    '<RT_I2T>': 16389,\n",
    "    \n",
    "    '<LT_TQA>': 16390,\n",
    "    '<RT_TQA>': 16391,\n",
    "    \n",
    "    '<LT_RLA>': 16401,\n",
    "    '<RT_RLA>': 16402,\n",
    "}\n",
    "for i in range(18):\n",
    "    SPC_TOKENS['ATARI_'+str(i)] = 16403 + i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = ruDolphApi(model, tokenizer, vae, bs=48)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import pickle\n",
    "#from fixed_replay_buffer import FixedReplayBuffer\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 123\n",
    "set_seed(seed)\n",
    "\n",
    "class RLDataset(Dataset):\n",
    "    spc_id = -1\n",
    "    \n",
    "    def __init__(self, states, actions, tokenizer, api, args):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.api = api\n",
    "        self.states = states\n",
    "        self.args = args\n",
    "        self.spc_tokens = SPC_TOKENS\n",
    "        \n",
    "        \n",
    "        self.actions = actions\n",
    "        #self.states = data\n",
    "        #self.data_path = data_path\n",
    "        #np.concatenate([np.array(self.hf_Frostbite[i]['obs']) for i in self.hf_Frostbite], axis=0)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.actions)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        \n",
    "        left_special_token = '<LT_RLA>'\n",
    "        right_special_token = '<RT_RLA>'\n",
    "        \n",
    "        lt = torch.zeros(self.args.l_text_seq_length,dtype=torch.int32)\n",
    "        lt[0] = 2\n",
    "        lt[1] = self.spc_tokens[left_special_token]\n",
    "        lt[2] = 3\n",
    "        \n",
    "        img = self.states[item]#self.states[item]\n",
    "        img = np.vstack((np.hstack((img[0],img[1])),np.hstack((img[2],img[3]))))\n",
    "        img = Image.fromarray(img)\n",
    "        img = self.api.image_transform(img)\n",
    "        img = img.unsqueeze(0).to(self.api.device)\n",
    "        img = self.api.vae.get_codebook_indices(img, disable_gumbel_softmax=True)[0]\n",
    "        \n",
    "        rt = torch.zeros(self.args.r_text_seq_length,dtype=torch.int32)\n",
    "        rt[0] = 2\n",
    "        rt[1] = self.spc_tokens[right_special_token]\n",
    "        rt[2] = self.spc_tokens['ATARI_{}'.format(self.actions[item])]\n",
    "        rt[3] = 3\n",
    "        \n",
    "        \n",
    "        #left_tokens = self.actions[item]\n",
    "        #right_tokens = self.states[item]\n",
    "        \n",
    "        return lt, img, rt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze(\n",
    "    model,\n",
    "    freeze_emb=False,\n",
    "    freeze_ln=False,\n",
    "    freeze_attn=True,\n",
    "    freeze_ff=True,\n",
    "    freeze_other=False,\n",
    "):\n",
    "    for name, p in model.module.named_parameters():\n",
    "        name = name.lower()\n",
    "        if 'ln' in name or 'norm' in name:\n",
    "            p.requires_grad = not freeze_ln\n",
    "        elif 'embeddings' in name:\n",
    "            p.requires_grad = not freeze_emb\n",
    "        elif 'mlp' in name:\n",
    "            p.requires_grad = not freeze_ff\n",
    "        elif 'attn' in name:\n",
    "            p.requires_grad = not freeze_attn\n",
    "        else:\n",
    "            p.requires_grad = not freeze_other\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(model, optimizer, losslogger, filename='checkpoint.pt'):\n",
    "    # Note: Input model & optimizer should be pre-defined.  This routine only updates their states.\n",
    "    start_epoch = 0\n",
    "    if os.path.isfile(filename):\n",
    "        print(\"=> loading checkpoint '{}'\".format(filename))\n",
    "        checkpoint = torch.load(filename)\n",
    "        start_epoch = checkpoint['epoch']\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        losslogger = checkpoint['losslogger']\n",
    "        print(\"=> loaded checkpoint '{}' (epoch {})\"\n",
    "                  .format(filename, checkpoint['epoch']))\n",
    "    else:\n",
    "        print(\"=> no checkpoint found at '{}'\".format(filename))\n",
    "\n",
    "    return model, optimizer, start_epoch, losslogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_s3_frame(item):\n",
    "    \n",
    "    left_special_token = '<LT_RLA>'\n",
    "    right_special_token = '<RT_RLA>'\n",
    "\n",
    "    lt = torch.zeros(args.l_text_seq_length,dtype=torch.int32)\n",
    "    lt[0] = 2\n",
    "    lt[1] = SPC_TOKENS[left_special_token]\n",
    "    lt[2] = 3\n",
    "\n",
    "    img = Image.open(io.BytesIO(item['data']))  \n",
    "    img = api.image_transform(img)\n",
    "    img = img.unsqueeze(0).to(api.device)\n",
    "    img = api.vae.get_codebook_indices(img, disable_gumbel_softmax=True)[0]\n",
    "\n",
    "    rt = torch.zeros(args.r_text_seq_length,dtype=torch.int32)\n",
    "    rt[0] = 2\n",
    "    rt[1] = SPC_TOKENS[right_special_token]\n",
    "    rt[2] = SPC_TOKENS['ATARI_{}'.format(item['action'])]\n",
    "    rt[3] = 3\n",
    "    return lt,img,rt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_options = {\n",
    "    \"anon\": False,\n",
    "    'key': 'officecds-user01',\n",
    "    'secret':'jym0FuboUnR5VsPmCgYTGv1QQfglYZhPRWbEfS59',\n",
    "    'client_kwargs': {\n",
    "        'endpoint_url':'https://s3pd12.sbercloud.ru'\n",
    "    }\n",
    "}\n",
    "s3dataset.init_webdataset(storage_options)\n",
    "game = 'Breakout'\n",
    "urls = ['s3://officecds-bucket01/datasets_v3/rl_atari_dataset/atari_{}/atari_{}_tr_{}.tar'.format(game.lower(),game.lower(),str(10000+i)[1:]) for i in range(1,500)]\n",
    "\n",
    "s3_dataset = wds.WebDataset(\n",
    "    urls, \n",
    "    handler=wds.warn_and_continue).shuffle(10000)\n",
    "s3_dataset = s3_dataset.map(preprocess_s3_frame)\n",
    "train_dataloader = DataLoader(s3_dataset, batch_size=args.bs, drop_last=True)\n",
    "batch = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.7.5+db37282)\n",
      "[Powered by Stella]\n"
     ]
    }
   ],
   "source": [
    "def take_fire_action(env):\n",
    "    \"\"\"Some games requires the agent to press 'FIRE' to start the game once loss a life.\"\"\"\n",
    "    assert env.unwrapped.get_action_meanings()[1] == 'FIRE'\n",
    "    s_t, _, _, _ = env.step(1)\n",
    "    return s_t\n",
    "\n",
    "def check_atari_env(env):\n",
    "    \"\"\"Check if is atari env and has fire action.\"\"\"\n",
    "    has_fire_action = False\n",
    "    lives = 0\n",
    "    try:\n",
    "        lives = env.ale.lives()\n",
    "        if env.unwrapped.get_action_meanings()[1] == 'FIRE':\n",
    "            has_fire_action = True\n",
    "    except AttributeError:\n",
    "        pass\n",
    "\n",
    "    return has_fire_action, lives\n",
    "\n",
    "eval_env = gym_env.create_atari_environment(\n",
    "                env_name=game,\n",
    "                screen_height=84,\n",
    "                screen_width=84,\n",
    "                frame_skip=4,\n",
    "                frame_stack=4,\n",
    "                max_episode_steps=28000,\n",
    "                seed=1,\n",
    "                noop_max=30,\n",
    "                terminal_on_life_loss=False,\n",
    "                clip_reward=False,\n",
    "            )\n",
    "\n",
    "def rudolph_step(observation,model):\n",
    "    left_special_token = '<LT_RLA>'\n",
    "    right_special_token = '<RT_RLA>'\n",
    "    \n",
    "    lt = torch.zeros(args.l_text_seq_length,dtype=torch.int32)\n",
    "    lt[0] = 2\n",
    "    lt[1] = SPC_TOKENS[left_special_token]\n",
    "    lt[2] = 3\n",
    "    rt = torch.zeros(2, dtype=torch.int32)\n",
    "    rt[0] = 2\n",
    "    rt[1] = SPC_TOKENS[right_special_token]\n",
    "    \n",
    "    img = np.vstack((np.hstack((observation[0],observation[1])),np.hstack((observation[2],observation[3]))))\n",
    "    img = Image.fromarray(img)\n",
    "    img = api.image_transform(img)\n",
    "    img = img.unsqueeze(0).to(api.device)\n",
    "    image_input_ids_text = api.vae.get_codebook_indices(img, disable_gumbel_softmax=True)[0]\n",
    "\n",
    "    attention_mask_text = get_attention_mask(1, args.l_text_seq_length,args.image_tokens_per_dim,2, args.device)\n",
    "    #attention_mask_text[:,:,args.l_text_seq_length:-args.r_text_seq_length,:]*=0\n",
    "    \n",
    "    input_ids_text = torch.cat((lt.to(args.device).unsqueeze(0), image_input_ids_text.to(args.device).unsqueeze(0), rt.to(args.device).unsqueeze(0)), dim=1)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits = model(input_ids_text, attention_mask_text)\n",
    "    # –ù–∏–∂–µ –∫–æ–¥, –≤ –∫–æ—Ç–æ—Ä–æ–º –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è —Å–µ–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–∑ –≤—ã–¥–∞–≤–∞–µ–º–æ–≥–æ –º–æ–¥–µ–ª—å—é —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è\n",
    "    #distribution = torch.softmax(logits[0][:, -1, SPC_TOKENS['ATARI_0']:SPC_TOKENS['ATARI_0']+4], 1)\n",
    "    #a_t = torch.multinomial(distribution, 1).item()\n",
    "    a_t = torch.argmax(logits[0][:, -1, SPC_TOKENS['ATARI_0']:SPC_TOKENS['ATARI_0']+4]).item()\n",
    "    return a_t\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_atari_reward(model):\n",
    "    observation = eval_env.reset()\n",
    "    should_fire, lives = check_atari_env(eval_env)\n",
    "    if should_fire:\n",
    "        observation = take_fire_action(eval_env)\n",
    "    sum_rewards = []\n",
    "\n",
    "    observation = eval_env.reset()\n",
    "    should_fire, lives = check_atari_env(eval_env)\n",
    "    if should_fire:\n",
    "        observation = take_fire_action(eval_env)\n",
    "    num_actions = eval_env.action_space.n\n",
    "\n",
    "    idd = 0; sum_reward = 0; frames = []\n",
    "    while True:\n",
    "        a_t = rudolph_step(observation,model)  \n",
    "        observation, reward, done, info = eval_env.step(a_t); idd+=1; first_step = False\n",
    "\n",
    "        sum_reward+=reward\n",
    "        # Take fire action after loss a life\n",
    "        if should_fire and not done and lives != info['lives']:\n",
    "            lives = info['lives']\n",
    "            observation = take_fire_action(eval_env)\n",
    "\n",
    "        if done:\n",
    "            sum_rewards.append(sum_reward)\n",
    "            print('Done with steps: ',idd,' Sum_reward: ',sum_reward)\n",
    "            return np.mean(sum_rewards)  \n",
    "    return np.mean(sum_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with steps:  120  Sum_reward:  0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_atari_reward(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, scheduler, args: Args, train_dataloader, from_checkpoints = False):\n",
    "    \"\"\"\n",
    "      args - arguments for training\n",
    "\n",
    "      train_dataloader - SQuADDataset class with text_q - text_a pairs in batch\n",
    "      \"\"\"\n",
    "    loss_logs = []\n",
    "    try:\n",
    "        t_steps = 10632*args.epochs#len(train_dataloader)*args.epochs\n",
    "        progress = tqdm(total = t_steps, desc='ü¶åü¶åü¶åfinetuning processü¶åü¶åü¶å')\n",
    "\n",
    "        save_counter = 0\n",
    "        start_epoch = 0\n",
    "\n",
    "        if from_checkpoints:\n",
    "            model, optimizer, start_epoch, loss_logs = load_checkpoint(model, optimizer, loss_logs, filename=os.path.join(args.save_path,f\"rudolph_sberquad_state_35500.pt\"))\n",
    "\n",
    "        for epoch in range(start_epoch, args.epochs):\n",
    "\n",
    "          for encoded_left_text, img_tokens, encoded_right_text in train_dataloader: #, prompt, word\n",
    "\n",
    "            bs_text = encoded_left_text.shape[0]\n",
    "\n",
    "            save_counter+=1\n",
    "\n",
    "            model.zero_grad()\n",
    "\n",
    "            image_seq_length = args.image_tokens_per_dim ** 2\n",
    "            total_seq_length = args.l_text_seq_length + image_seq_length + args.r_text_seq_length\n",
    "\n",
    "            attention_mask_text = get_attention_mask(bs_text, args.l_text_seq_length,\n",
    "                                                 args.image_tokens_per_dim, \n",
    "                                                 args.r_text_seq_length, args.device)\n",
    "            #attention_mask_text[:,:,args.l_text_seq_length+int(args.l_text_seq_length/2):-args.r_text_seq_length,:]*=0\n",
    "\n",
    "            image_input_ids_text = img_tokens#torch.zeros((bs_text, image_seq_length), dtype=torch.int32).to(args.device)\n",
    "\n",
    "            input_ids_text = torch.cat((encoded_left_text.to(args.device), image_input_ids_text.to(args.device), encoded_right_text.to(args.device)), dim=1)\n",
    "\n",
    "            loss, loss_values = model.forward(input_ids_text, attention_mask_text, \n",
    "                                                             lt_loss_weight=args.lt_loss_weight,\n",
    "                                                             img_loss_weight=args.img_loss_weight, \n",
    "                                                             rt_loss_weight=args.rt_loss_weight, return_loss=True)\n",
    "\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(),args.clip)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            if save_counter % args.save_every == 0:\n",
    "                print(f'Saving checkpoint here {args.model_name}_rudolph_{save_counter}.pt')\n",
    "                plt.plot(loss_logs)\n",
    "                plt.show()\n",
    "                state = {'epoch': epoch + 1, 'state_dict': model.state_dict(),\n",
    "                         'optimizer': optimizer.state_dict(), 'losslogger': loss_logs}\n",
    "                torch.save(\n",
    "                    model.state_dict(),\n",
    "                    os.path.join(args.save_path,f\"{args.model_name}rudolph_{save_counter}.pt\"))\n",
    "                \n",
    "                BREAKOUT_REWARD = get_atari_reward(model)\n",
    "                wandb.log({\"Reward\": BREAKOUT_REWARD})\n",
    "                #torch.save(\n",
    "                #    state,\n",
    "                #    os.path.join(args.save_path,f\"{args.model_name}state_{save_counter}.pt\"))\n",
    "            if args.wandb:\n",
    "                wandb.log({\"loss\":  loss.item()})\n",
    "            \n",
    "            loss_logs+=[loss.item()]\n",
    "            \n",
    "            wandb.log({\"loss\": loss.item()})\n",
    "            \n",
    "            progress.update()\n",
    "            progress.set_postfix({\"loss\": loss.item()})\n",
    "\n",
    "        print(f'Complitly tuned and saved here  {args.model_name}__textqa_last.pt')\n",
    "        plt.plot(loss_logs)\n",
    "        plt.show()\n",
    "        plt.savefig('rudolph_loss_13B_fromstart.png')\n",
    "        torch.save(\n",
    "            model.state_dict(),\n",
    "            os.path.join(args.save_path,f\"{args.model_name}textqa_last.pt\")\n",
    "        )\n",
    "        \n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "    \n",
    "    \n",
    "        print(f'What for did you stopped? Please change model_path to /{args.save_path}/{args.model_name}_rudolf_Failed_train')\n",
    "        plt.plot(loss_logs)\n",
    "        plt.show()\n",
    "    \n",
    "        torch.save(\n",
    "                model.state_dict(),\n",
    "                os.path.join(args.save_path,f\"{args.model_name}_rudolf_Failed_train.pt\")\n",
    "                )\n",
    "    except Exception as err:\n",
    "        print(f'Failed with {err}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "optimizer = bnb.optim.Adam8bit(model.parameters(), lr=args.lr)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer, max_lr=args.lr, final_div_factor=500, \n",
    "    steps_per_epoch=10632, epochs=args.epochs    #len(train_dataloader)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malstar\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.9 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.21"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/Aleksei_RL/RLdolph/jupyters/wandb/run-20230116_125225-174py0an</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/alstar/Gato/runs/174py0an\" target=\"_blank\">devout-haze-19</a></strong> to <a href=\"https://wandb.ai/alstar/Gato\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# '8909_CLIP_VAL_dense', reinit=True\n",
    "import os, wandb\n",
    "os.environ['WANDB_MODE'] = 'online'\n",
    "os.environ['WANDB_API_KEY'] = '77b33f530c461728a2fb12eeb694e04811d2d960'\n",
    "\n",
    "wandb.init(project=\"Gato\", entity=\"alstar\") #, id='23g9glh9', reinit=False, resume=True, config=args\n",
    "wandb.watch(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checkpoint_path = '../checkpoints/light_last.pt'\n",
    "checkpoint_path = '/home/jovyan/Aleksei_RL/inference/model/light_last.pt' \n",
    "#checkpoint_path = '/home/jovyan/Aleksei_RL/inference/model/350_S3_FBC/rudolph_sberquad_rudolph_106000.pt'\n",
    "checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "model.load_state_dict(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.013750791549682617,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "ü¶åü¶åü¶åfinetuning processü¶åü¶åü¶å",
       "rate": null,
       "total": 106320,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8d185534c1d4c91a3d1a44dcce072b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ü¶åü¶åü¶åfinetuning processü¶åü¶åü¶å:   0%|          | 0/106320 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = freeze(\n",
    "    model=model,\n",
    "    freeze_emb=False,\n",
    "    freeze_ln=False,\n",
    "    freeze_attn=True,\n",
    "    freeze_ff=True,\n",
    "    freeze_other=False,\n",
    ") \n",
    "\n",
    "\n",
    "train(model, optimizer, scheduler, args, train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
